{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Schema Matching and Mapping\n",
    "\n",
    "Let's start with Step 2.1 (Schema Matching).\n",
    "Step 2.1: Schema Matching (String Matching Techniques)\n",
    "\n",
    "The first task is to identify similarities between the different schema attributes. WE can use string matching techniques like Levenshtein distance and Jaccard similarity to find naming similarities.\n",
    "\n",
    "Levenshtein Distance measures how many single-character edits are needed to change one word into the other. It works well for detecting minor spelling differences in attribute names.\n",
    "\n",
    "Jaccard Similarity is a measure of similarity between two sets, used here to compare attribute names by treating them as sets of characters or tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Column 1           Column 2  Similarity Score\n",
      "3           billingid          billingid          1.000000\n",
      "10             Status             Status          1.000000\n",
      "7                name               name          1.000000\n",
      "4         billingdate        billingdate          1.000000\n",
      "15            Address            Address          1.000000\n",
      "19     policy_details     policy_details          1.000000\n",
      "13               Name               Name          1.000000\n",
      "14            Address            address          0.942857\n",
      "17            address            Address          0.942857\n",
      "11             Status             status          0.933333\n",
      "16             Status             status          0.933333\n",
      "1   contractstartdate  ContractStartDate          0.929412\n",
      "2     contractenddate    ContractEndDate          0.920000\n",
      "12               Name               name          0.900000\n",
      "6                name               Name          0.900000\n",
      "9            areatype           AreaType          0.900000\n",
      "18               name               Name          0.900000\n",
      "8                name               Name          0.900000\n",
      "0          contractid         ContractID          0.880000\n",
      "5            clientid           ClientID          0.850000\n"
     ]
    }
   ],
   "source": [
    "#fix and use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the merged CSV files into Pandas DataFrames\n",
    "files = {\n",
    "    \"mysql\": \"merged/merged_mysql.csv\",\n",
    "    \"postgres\": \"merged/merged_postgres.csv\",\n",
    "    \"web\": \"merged/merged_web.csv\",\n",
    "    \"xml\": \"merged/merged_xml.csv\",\n",
    "    \"cassandra\": \"merged/merged_cassandra.csv\",\n",
    "    \"mongo\": \"merged/merged_mongo.csv\"\n",
    "}\n",
    "\n",
    "dataframes = {key: pd.read_csv(\n",
    "    value, encoding='utf-8', low_memory=False) for key, value in files.items()}\n",
    "\n",
    "# Extract column names from each dataframe\n",
    "schemas = {key: list(df.columns) for key, df in dataframes.items()}\n",
    "\n",
    "# Flatten schema into a list of (source, column_name)\n",
    "schema_list = [(source, col)\n",
    "               for source, columns in schemas.items() for col in columns]\n",
    "column_names = [col for _, col in schema_list]\n",
    "\n",
    "# Compute pairwise Levenshtein distance between column names\n",
    "\n",
    "\n",
    "def compute_levenshtein_similarity(col1, col2):\n",
    "    return 1 - (levenshtein_distance(col1, col2) / max(len(col1), len(col2)))\n",
    "\n",
    "\n",
    "levenshtein_scores = np.zeros((len(column_names), len(column_names)))\n",
    "\n",
    "for i in range(len(column_names)):\n",
    "    for j in range(len(column_names)):\n",
    "        levenshtein_scores[i, j] = compute_levenshtein_similarity(\n",
    "            column_names[i], column_names[j])\n",
    "\n",
    "# Compute Jaccard similarity between column names\n",
    "\n",
    "\n",
    "def jaccard_similarity(str1, str2):\n",
    "    set1, set2 = set(str1.lower()), set(str2.lower())\n",
    "    return len(set1 & set2) / len(set1 | set2)\n",
    "\n",
    "\n",
    "jaccard_scores = np.zeros((len(column_names), len(column_names)))\n",
    "\n",
    "for i in range(len(column_names)):\n",
    "    for j in range(len(column_names)):\n",
    "        jaccard_scores[i, j] = jaccard_similarity(\n",
    "            column_names[i], column_names[j])\n",
    "\n",
    "# Compute TF-IDF cosine similarity\n",
    "vectorizer = TfidfVectorizer().fit_transform(column_names)\n",
    "cosine_sim_matrix = cosine_similarity(vectorizer, vectorizer)\n",
    "\n",
    "# Combine all similarity metrics (weighted sum)\n",
    "final_similarity_scores = (levenshtein_scores * 0.4) + \\\n",
    "    (jaccard_scores * 0.3) + (cosine_sim_matrix * 0.3)\n",
    "\n",
    "# Create a DataFrame for similarity results\n",
    "similarity_df = pd.DataFrame(\n",
    "    final_similarity_scores, index=column_names, columns=column_names)\n",
    "\n",
    "# Print highly similar column pairs (threshold > 0.7)\n",
    "pairs = []\n",
    "threshold = 0.7\n",
    "for i in range(len(column_names)):\n",
    "    for j in range(i + 1, len(column_names)):\n",
    "        if final_similarity_scores[i, j] > threshold:\n",
    "            pairs.append(\n",
    "                (column_names[i], column_names[j], final_similarity_scores[i, j]))\n",
    "\n",
    "similar_columns = pd.DataFrame(\n",
    "    pairs, columns=[\"Column 1\", \"Column 2\", \"Similarity Score\"])\n",
    "print(similar_columns.sort_values(by=\"Similarity Score\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2.2: Schema Mapping (GAV, LAV, GLAV)\n",
    "\n",
    "Once we’ve matched schemas using string similarity, we’ll need to define mappings between the local schemas and a global schema. For the sake of this example, let's assume we're going to define a simple global schema.\n",
    "GAV (Global-as-View):\n",
    "\n",
    "For this, we define a global schema as views over source schemas and transform data to fit the global schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL Columns: ['customerid', 'customername', 'contractid', 'statusid', 'contractstartdate', 'contractenddate', 'statusname', 'policyid', 'policyname', 'policyinfo', 'publicationdate', 'effectivedate', 'fineid', 'reasonid', 'reasondescription', 'usageid', 'departmentid', 'sourceid', 'usagedate', 'volumeused', 'purpose', 'departmentname', 'equipmentid', 'equipmentname', 'lastinspectiondate', 'logid', 'maintenancedate', 'partsreplaced', 'techniciannotes', 'billingid', 'billingdate', 'amountdue', 'paymentstatus']\n",
      "PostgreSQL Columns: ['clientid', 'name', 'fulladdress', 'postcode', 'areatypeid', 'mobileinfo', 'areatype', 'description_x', 'billingid', 'billingdate', 'totalamount', 'billingdetailid', 'wastecategoryid', 'quantityinkg_x', 'subtotal', 'wastetype', 'unitpriceperkg', 'description_y', 'frequencyid', 'lastcollectiondate', 'nextcollectiondate', 'collectionfrequency', 'description', 'disposalid', 'disposaldate', 'quantityinkg_y']\n",
      "Cassandra Columns: ['client_id', 'address', 'area_type_id_merged', 'classification_id', 'client_status_id', 'contact_number', 'name', 'area_type', 'description_merged', 'classification', 'description_classification', 'account_status', 'description_contract', 'contract_id', 'area_type_id_contract', 'base_fee', 'contract_type_id', 'end_date', 'last_updated', 'reason_for_status', 'start_date', 'status_id', 'description_contract_status', 'status', 'policy_id', 'effective_date', 'policy_details', 'policy_name']\n",
      "MongoDB Columns: ['_id', 'Name', 'ConsumerType', 'Address', 'ContactInfo', 'RelevanceID', 'ContractID', 'ContractStartDate', 'ContractEndDate', 'ContractCommitment', 'Comments', 'optimization_data', 'participation_data', 'contract_status_data', 'policy_data', 'policy_details', 'program_data', 'optimization_type_data', 'status_data', 'relevance_data', 'program_type_data']\n",
      "XML Columns: ['ClientID', 'Name', 'Contact', 'Address', 'AreaType', 'ComplaintID', 'Type', 'Date', 'Status', 'IssueID', 'IssueType', 'BreachID', 'Reason']\n",
      "Web Columns: ['ReportID', 'Validation', 'Time', 'ReservoirID', 'Temperature', 'Turbidity', 'pH', 'ReportURL', 'SensorID', 'SensorType', 'Location', 'InstallationDate', 'Status', 'pHDataID', 'Report_ReportID', 'TempRecordDataID', 'TurbidityDataID']\n",
      "     DB1     Column1         DB2             Column2  Levenshtein   Jaccard\n",
      "0  MySQL  customerid  PostgreSQL            postcode           44  0.545455\n",
      "1  MySQL  customerid  PostgreSQL       description_x           26  0.571429\n",
      "2  MySQL  customerid  PostgreSQL     wastecategoryid           48  0.571429\n",
      "3  MySQL  customerid  PostgreSQL       description_y           26  0.571429\n",
      "4  MySQL  customerid  PostgreSQL  lastcollectiondate           43  0.538462\n",
      "GAV Queries: [\"SELECT CustomerID, CustomerName, Contact, Address FROM GlobalSchema WHERE Status='Active'\", 'SELECT ContractID, ContractStartDate, ContractEndDate FROM GlobalSchema WHERE BillingAmount > 1000', 'SELECT CustomerID, BillingAmount FROM GlobalSchema ORDER BY BillingAmount DESC']\n",
      "LAV Queries: [\"SELECT clientid as clientid, name as name, mobileinfo as mobileinfo FROM PostgreSQL WHERE areatype='Residential'\", \"SELECT contract_id AS ContractID, start_date AS ContractStartDate FROM MySQL WHERE region='South'\", \"SELECT _id AS clientid, address AS Address FROM MongoDB WHERE customer_type='Premium'\"]\n",
      "GLAV Queries: ['SELECT c.clientid AS CustomerID, c.name AS CustomerName, c.contact_number AS Contact FROM Cassandra c JOIN MongoDB m ON c.client_id = m._id', 'SELECT p.clientid AS CustomerID, p.name AS CustomerName FROM PostgreSQL p JOIN MySQL m ON p.clientid = m.client_id', 'SELECT x.id AS CustomerID, x.address AS Address FROM XML x JOIN Web w ON x.id = w.user_id']\n",
      "Optimized Queries: [\"SELECT CustomerID, CustomerName, Contact, Address FROM GlobalSchema WHERE Status='Active' /* Optimized Execution Plan */\", 'SELECT ContractID, ContractStartDate, ContractEndDate FROM GlobalSchema WHERE BillingAmount > 1000 /* Optimized Execution Plan */', 'SELECT CustomerID, BillingAmount FROM GlobalSchema ORDER BY BillingAmount DESC /* Optimized Execution Plan */', \"SELECT clientid as clientid, name as name, mobileinfo as mobileinfo FROM PostgreSQL WHERE areatype='Residential' /* Optimized Execution Plan */\", \"SELECT contract_id AS ContractID, start_date AS ContractStartDate FROM MySQL WHERE region='South' /* Optimized Execution Plan */\", \"SELECT _id AS clientid, address AS Address FROM MongoDB WHERE customer_type='Premium' /* Optimized Execution Plan */\", 'SELECT c.clientid AS CustomerID, c.name AS CustomerName, c.contact_number AS Contact FROM Cassandra c JOIN MongoDB m ON c.client_id = m._id /* Optimized Execution Plan */', 'SELECT p.clientid AS CustomerID, p.name AS CustomerName FROM PostgreSQL p JOIN MySQL m ON p.clientid = m.client_id /* Optimized Execution Plan */', 'SELECT x.id AS CustomerID, x.address AS Address FROM XML x JOIN Web w ON x.id = w.user_id /* Optimized Execution Plan */']\n",
      "Translated NoSQL Queries: [\"db.find({ CustomerID, CustomerName, Contact, Address }) GlobalSchema WHERE Status='Active'\", 'db.find({ ContractID, ContractStartDate, ContractEndDate }) GlobalSchema WHERE BillingAmount > 1000', 'db.find({ CustomerID, BillingAmount }) GlobalSchema ORDER BY BillingAmount DESC']\n",
      "Translated SQL Queries: ['SELECT c.clientid AS CustomerID, c.name AS CustomerName, c.contact_number AS Contact FROM Cassandra c JOIN MongoDB m ON c.client_id = m._id', 'SELECT p.clientid AS CustomerID, p.name AS CustomerName FROM PostgreSQL p JOIN MySQL m ON p.clientid = m.client_id', 'SELECT x.id AS CustomerID, x.address AS Address FROM XML x JOIN Web w ON x.id = w.user_id']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamro\\AppData\\Local\\Temp\\ipykernel_4508\\1826893823.py:8: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mysql_df = pd.read_csv('merged/merged_mysql.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load merged CSV files\n",
    "mysql_df = pd.read_csv('merged/merged_mysql.csv')\n",
    "postgres_df = pd.read_csv('merged/merged_postgres.csv')\n",
    "cassandra_df = pd.read_csv('merged/merged_cassandra.csv')\n",
    "mongo_df = pd.read_csv('merged/merged_mongo.csv')\n",
    "xml_df = pd.read_csv('merged/merged_xml.csv')\n",
    "web_df = pd.read_csv('merged/merged_web.csv')\n",
    "# Print column names for debugging\n",
    "print(\"MySQL Columns:\", mysql_df.columns.tolist())\n",
    "print(\"PostgreSQL Columns:\", postgres_df.columns.tolist())\n",
    "print(\"Cassandra Columns:\", cassandra_df.columns.tolist())\n",
    "print(\"MongoDB Columns:\", mongo_df.columns.tolist())\n",
    "print(\"XML Columns:\", xml_df.columns.tolist())\n",
    "print(\"Web Columns:\", web_df.columns.tolist())\n",
    "\n",
    "# Extract column names\n",
    "schemas = {\n",
    "    \"MySQL\": mysql_df.columns,\n",
    "    \"PostgreSQL\": postgres_df.columns,\n",
    "    \"Cassandra\": cassandra_df.columns,\n",
    "    \"MongoDB\": mongo_df.columns,\n",
    "    \"XML\": xml_df.columns,\n",
    "    \"Web\": web_df.columns\n",
    "}\n",
    "\n",
    "# Function to compute similarity scores\n",
    "\n",
    "\n",
    "def compute_similarity(col1, col2):\n",
    "    lev_score = fuzz.ratio(col1, col2)\n",
    "    jac_score = len(set(col1).intersection(set(col2))) / \\\n",
    "        len(set(col1).union(set(col2)))\n",
    "    return lev_score, jac_score\n",
    "\n",
    "\n",
    "# Schema Matching\n",
    "matched_schemas = []\n",
    "for db1, cols1 in schemas.items():\n",
    "    for db2, cols2 in schemas.items():\n",
    "        if db1 != db2:\n",
    "            for col1 in cols1:\n",
    "                for col2 in cols2:\n",
    "                    lev_score, jac_score = compute_similarity(col1, col2)\n",
    "                    if lev_score > 80 or jac_score > 0.5:\n",
    "                        matched_schemas.append(\n",
    "                            (db1, col1, db2, col2, lev_score, jac_score))\n",
    "\n",
    "# Convert to DataFrame\n",
    "matched_df = pd.DataFrame(matched_schemas, columns=[\n",
    "                          \"DB1\", \"Column1\", \"DB2\", \"Column2\", \"Levenshtein\", \"Jaccard\"])\n",
    "print(matched_df.head())\n",
    "\n",
    "# Define a mediated global schema\n",
    "global_schema = {\n",
    "    \"CustomerID\": [\"customerid\", \"clientid\", \"ClientID\", \"client_id\", \"_id\"],\n",
    "    \"CustomerName\": [\"customername\", \"name\", \"Name\"],\n",
    "    \"Contact\": [\"mobileinfo\", \"contact_number\", \"ContactInfo\"],\n",
    "    \"Address\": [\"fulladdress\", \"Address\", \"address\"],\n",
    "    \"ContractID\": [\"contractid\", \"ContractID\", \"contract_id\"],\n",
    "    \"ContractStartDate\": [\"contractstartdate\", \"ContractStartDate\", \"start_date\"],\n",
    "    \"ContractEndDate\": [\"contractenddate\", \"ContractEndDate\", \"end_date\"],\n",
    "    \"BillingAmount\": [\"totalamount\", \"amountdue\", \"paymentstatus\"],\n",
    "    \"Status\": [\"statusid\", \"status\", \"statusname\", \"classification\"],\n",
    "    \"AreaType\": [\"areatype\", \"AreaType\"],\n",
    "    \"Region\": [\"region\", \"Region\"],  # Ensuring Region exists\n",
    "    \"WaterResourceID\": [\"sourceid\", \"ReservoirID\"],\n",
    "    \"Waste\": [\"wastetype\", \"quantityinkg_x\", \"quantityinkg_y\", \"WasteQuantity\", \"WasteType\"],\n",
    "    \"CustomerType\": [\"ConsumerType\", \"customer_type\"],\n",
    "    \"Turbidity\": [\"turbidity\", \"Turbidity\"],\n",
    "    \"phlevel\": [\"pH\"],\n",
    "    \"Reservoir\": [\"Location\"],\n",
    "    \"Reason\": [\"reasondescription\"]\n",
    "}\n",
    "# GAV Mapping (Global-as-View)\n",
    "\n",
    "\n",
    "def GAV_query():\n",
    "    return \"SELECT CustomerID, CustomerName, Contact, Address FROM GlobalSchema WHERE Status='Active'\"\n",
    "\n",
    "\n",
    "def GAV_query2():\n",
    "    return \"SELECT ContractID, ContractStartDate, ContractEndDate FROM GlobalSchema WHERE BillingAmount > 1000\"\n",
    "\n",
    "\n",
    "def GAV_query3():\n",
    "    return \"SELECT CustomerID, BillingAmount FROM GlobalSchema ORDER BY BillingAmount DESC\"\n",
    "\n",
    "\n",
    "gav_queries = [GAV_query(), GAV_query2(), GAV_query3()]\n",
    "print(\"GAV Queries:\", gav_queries)\n",
    "\n",
    "# LAV Mapping (Local-as-View)\n",
    "\n",
    "\n",
    "def LAV_query():\n",
    "    return \"SELECT clientid as CustomerID, name as CustomerName, mobileinfo as Contact FROM PostgreSQL WHERE areatype='Residential'\"\n",
    "\n",
    "\n",
    "def LAV_query2():\n",
    "    return \"SELECT contract_id AS ContractID, start_date AS ContractStartDate FROM MySQL WHERE region='South'\"\n",
    "\n",
    "\n",
    "def LAV_query3():\n",
    "    return \"SELECT _id AS CustomerID, address AS Address FROM MongoDB WHERE customer_type='Premium'\"\n",
    "\n",
    "\n",
    "def unfold_LAV(query):\n",
    "    # Example function to translate global schema query into source-specific queries\n",
    "    return query.replace(\"CustomerID\", \"clientid\").replace(\"CustomerName\", \"name\").replace(\"Contact\", \"mobileinfo\")\n",
    "\n",
    "\n",
    "lav_queries = [unfold_LAV(LAV_query()), unfold_LAV(\n",
    "    LAV_query2()), unfold_LAV(LAV_query3())]\n",
    "print(\"LAV Queries:\", lav_queries)\n",
    "\n",
    "# GLAV Mapping (Global-Local-As-View)\n",
    "\n",
    "\n",
    "def GLAV_query():\n",
    "    return \"SELECT c.clientid AS CustomerID, c.name AS CustomerName, c.contact_number AS Contact FROM Cassandra c JOIN MongoDB m ON c.client_id = m._id\"\n",
    "\n",
    "\n",
    "def GLAV_query2():\n",
    "    return \"SELECT p.clientid AS CustomerID, p.name AS CustomerName FROM PostgreSQL p JOIN MySQL m ON p.clientid = m.client_id\"\n",
    "\n",
    "\n",
    "def GLAV_query3():\n",
    "    return \"SELECT x.id AS CustomerID, x.address AS Address FROM XML x JOIN Web w ON x.id = w.user_id\"\n",
    "\n",
    "\n",
    "glav_queries = [GLAV_query(), GLAV_query2(), GLAV_query3()]\n",
    "print(\"GLAV Queries:\", glav_queries)\n",
    "\n",
    "# Query Processing and Optimization (Example)\n",
    "\n",
    "\n",
    "def optimize_query(query):\n",
    "    return query + \" /* Optimized Execution Plan */\"\n",
    "\n",
    "\n",
    "optimized_queries = [optimize_query(\n",
    "    q) for q in gav_queries + lav_queries + glav_queries]\n",
    "print(\"Optimized Queries:\", optimized_queries)\n",
    "\n",
    "# Wrapper functions for query translation\n",
    "\n",
    "\n",
    "def sql_to_nosql(sql_query):\n",
    "    return sql_query.replace(\"SELECT\", \"db.find({\").replace(\"FROM\", \"})\")\n",
    "\n",
    "\n",
    "def jsonpath_to_sql(json_query):\n",
    "    return json_query.replace(\"$.\", \"SELECT * FROM\")\n",
    "\n",
    "\n",
    "translated_nosql_queries = [sql_to_nosql(q) for q in gav_queries]\n",
    "translated_sql_queries = [jsonpath_to_sql(q) for q in glav_queries]\n",
    "print(\"Translated NoSQL Queries:\", translated_nosql_queries)\n",
    "print(\"Translated SQL Queries:\", translated_sql_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL Columns: ['customerid', 'customername', 'contractid', 'statusid', 'contractstartdate', 'contractenddate', 'statusname', 'policyid', 'policyname', 'policyinfo', 'publicationdate', 'effectivedate', 'fineid', 'reasonid', 'reasondescription', 'usageid', 'departmentid', 'sourceid', 'usagedate', 'volumeused', 'purpose', 'departmentname', 'equipmentid', 'equipmentname', 'lastinspectiondate', 'logid', 'maintenancedate', 'partsreplaced', 'techniciannotes', 'billingid', 'billingdate', 'amountdue', 'paymentstatus']\n",
      "PostgreSQL Columns: ['clientid', 'name', 'fulladdress', 'postcode', 'areatypeid', 'mobileinfo', 'areatype', 'description_x', 'billingid', 'billingdate', 'totalamount', 'billingdetailid', 'wastecategoryid', 'quantityinkg_x', 'subtotal', 'wastetype', 'unitpriceperkg', 'description_y', 'frequencyid', 'lastcollectiondate', 'nextcollectiondate', 'collectionfrequency', 'description', 'disposalid', 'disposaldate', 'quantityinkg_y']\n",
      "Cassandra Columns: ['client_id', 'address', 'area_type_id_merged', 'classification_id', 'client_status_id', 'contact_number', 'name', 'area_type', 'description_merged', 'classification', 'description_classification', 'account_status', 'description_contract', 'contract_id', 'area_type_id_contract', 'base_fee', 'contract_type_id', 'end_date', 'last_updated', 'reason_for_status', 'start_date', 'status_id', 'description_contract_status', 'status', 'policy_id', 'effective_date', 'policy_details', 'policy_name']\n",
      "MongoDB Columns: ['_id', 'Name', 'ConsumerType', 'Address', 'ContactInfo', 'RelevanceID', 'ContractID', 'ContractStartDate', 'ContractEndDate', 'ContractCommitment', 'Comments', 'optimization_data', 'participation_data', 'contract_status_data', 'policy_data', 'policy_details', 'program_data', 'optimization_type_data', 'status_data', 'relevance_data', 'program_type_data']\n",
      "XML Columns: ['ClientID', 'Name', 'Contact', 'Address', 'AreaType', 'ComplaintID', 'Type', 'Date', 'Status', 'IssueID', 'IssueType', 'BreachID', 'Reason']\n",
      "Web Columns: ['ReportID', 'Validation', 'Time', 'ReservoirID', 'Temperature', 'Turbidity', 'pH', 'ReportURL', 'SensorID', 'SensorType', 'Location', 'InstallationDate', 'Status', 'pHDataID', 'Report_ReportID', 'TempRecordDataID', 'TurbidityDataID']\n",
      "\n",
      "Executing GAV Queries:\n",
      "\n",
      "Available columns in DataFrame: ['clientid', 'name', 'fulladdress', 'postcode', 'areatypeid', 'mobileinfo', 'areatype', 'description_x', 'billingid', 'billingdate', 'totalamount', 'billingdetailid', 'wastecategoryid', 'quantityinkg_x', 'subtotal', 'wastetype', 'unitpriceperkg', 'description_y', 'frequencyid', 'lastcollectiondate', 'nextcollectiondate', 'collectionfrequency', 'description', 'disposalid', 'disposaldate', 'quantityinkg_y']\n",
      "Filtering on wastetype for value: Organic\n",
      "          name     areatype\n",
      "0   Customer_1  Residential\n",
      "4   Customer_2  Residential\n",
      "8   Customer_3  Residential\n",
      "12  Customer_4  Residential\n",
      "16  Customer_5  Residential\n",
      "\n",
      "Available columns in DataFrame: ['customerid', 'customername', 'contractid', 'statusid', 'contractstartdate', 'contractenddate', 'statusname', 'policyid', 'policyname', 'policyinfo', 'publicationdate', 'effectivedate', 'fineid', 'reasonid', 'reasondescription', 'usageid', 'departmentid', 'sourceid', 'usagedate', 'volumeused', 'purpose', 'departmentname', 'equipmentid', 'equipmentname', 'lastinspectiondate', 'logid', 'maintenancedate', 'partsreplaced', 'techniciannotes', 'billingid', 'billingdate', 'amountdue', 'paymentstatus']\n",
      "Filtering on amountdue for value: 75.5\n",
      "   contractid contractstartdate contractenddate\n",
      "0           1        2020-02-14      2021-02-13\n",
      "1           1        2020-02-14      2021-02-13\n",
      "2           1        2020-02-14      2021-02-13\n",
      "3           1        2020-02-14      2021-02-13\n",
      "4           1        2020-02-14      2021-02-13\n",
      "\n",
      "Available columns in DataFrame: ['clientid', 'name', 'fulladdress', 'postcode', 'areatypeid', 'mobileinfo', 'areatype', 'description_x', 'billingid', 'billingdate', 'totalamount', 'billingdetailid', 'wastecategoryid', 'quantityinkg_x', 'subtotal', 'wastetype', 'unitpriceperkg', 'description_y', 'frequencyid', 'lastcollectiondate', 'nextcollectiondate', 'collectionfrequency', 'description', 'disposalid', 'disposaldate', 'quantityinkg_y']\n",
      "             name  areatype  totalamount\n",
      "783  Customer_196  Business          5.0\n",
      "782  Customer_196  Business          5.0\n",
      "781  Customer_196  Business          5.0\n",
      "780  Customer_196  Business          5.0\n",
      "779  Customer_195  Business          5.0\n",
      "\n",
      "Executing LAV Queries:\n",
      "\n",
      "Available columns in DataFrame: ['clientid', 'name', 'fulladdress', 'postcode', 'areatypeid', 'mobileinfo', 'areatype', 'description_x', 'billingid', 'billingdate', 'totalamount', 'billingdetailid', 'wastecategoryid', 'quantityinkg_x', 'subtotal', 'wastetype', 'unitpriceperkg', 'description_y', 'frequencyid', 'lastcollectiondate', 'nextcollectiondate', 'collectionfrequency', 'description', 'disposalid', 'disposaldate', 'quantityinkg_y']\n",
      "Filtering on areatype for value: residential\n",
      "   clientid        name   mobileinfo   wastetype\n",
      "0         1  Customer_1  35191000001     organic\n",
      "1         1  Customer_1  35191000001   hazardous\n",
      "2         1  Customer_1  35191000001  recyclable\n",
      "3         1  Customer_1  35191000001     general\n",
      "4         2  Customer_2  35191000002     organic\n",
      "\n",
      "Available columns in DataFrame: ['customerid', 'customername', 'contractid', 'statusid', 'contractstartdate', 'contractenddate', 'statusname', 'policyid', 'policyname', 'policyinfo', 'publicationdate', 'effectivedate', 'fineid', 'reasonid', 'reasondescription', 'usageid', 'departmentid', 'sourceid', 'usagedate', 'volumeused', 'purpose', 'departmentname', 'equipmentid', 'equipmentname', 'lastinspectiondate', 'logid', 'maintenancedate', 'partsreplaced', 'techniciannotes', 'billingid', 'billingdate', 'amountdue', 'paymentstatus']\n",
      "Filtering on reasondescription for value: Late Payment\n",
      "  customername  contractid contractstartdate\n",
      "0   Customer_1           1        2020-02-14\n",
      "1   Customer_1           1        2020-02-14\n",
      "2   Customer_1           1        2020-02-14\n",
      "3   Customer_1           1        2020-02-14\n",
      "4   Customer_1           1        2020-02-14\n",
      "\n",
      "Executing LAV Query for Average Turbidity per Reservoir:\n",
      "      Location  Turbidity\n",
      "0  Reservoir_1   4.320000\n",
      "1  Reservoir_3   4.228496\n",
      "2  Reservoir_5   4.065510\n",
      "\n",
      "Executing GLAV Queries:\n",
      "GLAV2 Debug Columns: ['clientid', 'name', 'fulladdress', 'postcode', 'areatypeid', 'mobileinfo', 'areatype', 'description_x', 'billingid_x', 'billingdate_x', 'totalamount', 'billingdetailid', 'wastecategoryid', 'quantityinkg_x', 'subtotal', 'wastetype', 'unitpriceperkg', 'description_y', 'frequencyid', 'lastcollectiondate', 'nextcollectiondate', 'collectionfrequency', 'description', 'disposalid', 'disposaldate', 'quantityinkg_y', 'customerid', 'customername', 'contractid', 'statusid', 'contractstartdate', 'contractenddate', 'statusname', 'policyid', 'policyname', 'policyinfo', 'publicationdate', 'effectivedate', 'fineid', 'reasonid', 'reasondescription', 'usageid', 'departmentid', 'sourceid', 'usagedate', 'volumeused', 'purpose', 'departmentname', 'equipmentid', 'equipmentname', 'lastinspectiondate', 'logid', 'maintenancedate', 'partsreplaced', 'techniciannotes', 'billingid_y', 'billingdate_y', 'amountdue', 'paymentstatus']\n",
      "   customerid customername\n",
      "0           1   Customer_1\n",
      "1           1   Customer_1\n",
      "2           1   Customer_1\n",
      "3           1   Customer_1\n",
      "4           1   Customer_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamro\\AppData\\Local\\Temp\\ipykernel_4508\\1516180418.py:6: DtypeWarning: Columns (14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mysql_df = pd.read_csv('merged/merged_mysql.csv')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' # Find actual column names before merging\\nxml_customer_col = get_actual_column(xml_df, \"CustomerID\")\\nweb_customer_col = get_actual_column(web_df, \"CustomerID\")\\n\\n# Check if both columns exist before merging\\nif xml_customer_col and web_customer_col:\\n    glav3 = pd.merge(xml_df, web_df, left_on=xml_customer_col,\\n                     right_on=web_customer_col, how=\"inner\")\\n    print(\"GLAV3 Debug Columns:\", glav3.columns.tolist())\\n\\n    # Select columns dynamically\\n    actual_cols3 = [get_actual_column(glav3, col) for col in [\\n        \"CustomerID\", \"Address\"] if get_actual_column(glav3, col)]\\n    if actual_cols3:\\n        print(glav3[actual_cols3].head())\\n    else:\\n        print(\"GLAV3: No matching columns found.\")\\nelse:\\n    print(\"\\n GLAV3 Merge Skipped: Could not find common columns in XML and Web DataFrames!\")\\n    print(f\"XML Customer Column: {xml_customer_col}, Web Customer Column: {web_customer_col}\") '"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fix and use\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Load CSVs\n",
    "mysql_df = pd.read_csv('merged/merged_mysql.csv')\n",
    "postgres_df = pd.read_csv('merged/merged_postgres.csv')\n",
    "cassandra_df = pd.read_csv('merged/merged_cassandra.csv')\n",
    "mongo_df = pd.read_csv('merged/merged_mongo.csv')\n",
    "xml_df = pd.read_csv('merged/merged_xml.csv')\n",
    "web_df = pd.read_csv('merged/merged_web.csv')\n",
    "\n",
    "# Print column names for debugging\n",
    "print(\"MySQL Columns:\", mysql_df.columns.tolist())\n",
    "print(\"PostgreSQL Columns:\", postgres_df.columns.tolist())\n",
    "print(\"Cassandra Columns:\", cassandra_df.columns.tolist())\n",
    "print(\"MongoDB Columns:\", mongo_df.columns.tolist())\n",
    "print(\"XML Columns:\", xml_df.columns.tolist())\n",
    "print(\"Web Columns:\", web_df.columns.tolist())\n",
    "\n",
    "# Updated Global Schema Mapping (with correct column names)\n",
    "global_schema = {\n",
    "    \"CustomerID\": [\"customerid\", \"clientid\", \"ClientID\", \"client_id\", \"_id\"],\n",
    "    \"CustomerName\": [\"customername\", \"name\", \"Name\"],\n",
    "    \"Contact\": [\"mobileinfo\", \"contact_number\", \"ContactInfo\"],\n",
    "    \"Address\": [\"fulladdress\", \"Address\", \"address\"],\n",
    "    \"ContractID\": [\"contractid\", \"ContractID\", \"contract_id\"],\n",
    "    \"ContractStartDate\": [\"contractstartdate\", \"ContractStartDate\", \"start_date\"],\n",
    "    \"ContractEndDate\": [\"contractenddate\", \"ContractEndDate\", \"end_date\"],\n",
    "    \"BillingAmount\": [\"totalamount\", \"amountdue\", \"paymentstatus\"],\n",
    "    \"Status\": [\"statusid\", \"status\", \"statusname\", \"classification\"],\n",
    "    \"AreaType\": [\"areatype\", \"AreaType\"],\n",
    "    \"Region\": [\"region\", \"Region\"],  # Ensuring Region exists\n",
    "    \"WaterResourceID\": [\"sourceid\", \"ReservoirID\"],\n",
    "    \"Waste\": [\"wastetype\",\"quantityinkg_x\", \"quantityinkg_y\", \"WasteQuantity\", \"WasteType\"],\n",
    "    \"CustomerType\": [\"ConsumerType\", \"customer_type\"],\n",
    "    \"Turbidity\": [\"turbidity\", \"Turbidity\"],\n",
    "    \"phlevel\": [\"pH\"],\n",
    "    \"Reservoir\":[\"Location\"],\n",
    "    \"Reason\": [\"reasondescription\"]\n",
    "}\n",
    "\n",
    "# Function to map global schema fields to actual column names in CSV\n",
    "\n",
    "\n",
    "def get_actual_column(df, global_column):\n",
    "    for possible_col in global_schema.get(global_column, []):\n",
    "        if possible_col in df.columns:\n",
    "            return possible_col\n",
    "    return None  # If no match is found\n",
    "\n",
    "# Function to execute queries on CSV DataFrames\n",
    "\n",
    "\n",
    "def execute_query(df, select_cols, where_condition=None, order_by=None):\n",
    "    try:\n",
    "        print(f\"\\nAvailable columns in DataFrame: {df.columns.tolist()}\")\n",
    "\n",
    "        # Map global schema columns to actual columns\n",
    "        selected_cols = [get_actual_column(\n",
    "            df, col) for col in select_cols if get_actual_column(df, col)]\n",
    "\n",
    "        if not selected_cols:\n",
    "            print(\"No valid columns found for selection.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Filter DataFrame\n",
    "        if where_condition:\n",
    "            col, value = where_condition\n",
    "            actual_col = get_actual_column(df, col)\n",
    "            if actual_col:\n",
    "                print(f\"Filtering on {actual_col} for value: {value}\")\n",
    "                df[actual_col] = df[actual_col].astype(\n",
    "                    str).str.lower()  # Normalize case\n",
    "                df = df[df[actual_col] == str(value).lower()]\n",
    "            else:\n",
    "                print(f\"Warning: Column {col} not found for filtering. Skipping filter.\")\n",
    "\n",
    "        # Order By\n",
    "        if order_by:\n",
    "            actual_col = get_actual_column(df, order_by)\n",
    "            if actual_col:\n",
    "                df = df.sort_values(by=actual_col, ascending=False)\n",
    "            else:\n",
    "                print(f\"Warning: Column {order_by} not found for ordering. Skipping order by.\")\n",
    "\n",
    "        # Select only required columns\n",
    "        df = df[selected_cols]\n",
    "\n",
    "        return df.head()  # Return first 5 results for preview\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Simulate GAV queries with correct column names\n",
    "print(\"\\nExecuting GAV Queries:\")\n",
    "print(execute_query(postgres_df, [\"CustomerName\", \"AreaType\"],\n",
    "      (\"Waste\", \"Organic\")))\n",
    "print(execute_query(mysql_df, [\n",
    "      \"ContractID\", \"ContractStartDate\", \"ContractEndDate\"], (\"BillingAmount\", \"75.5\")))\n",
    "print(execute_query(postgres_df, [\"CustomerName\",\n",
    "      \"AreaType\", \"BillingAmount\"], order_by=\"BillingAmount\"))\n",
    "\n",
    "# Simulate LAV queries with corrected filters\n",
    "print(\"\\nExecuting LAV Queries:\")\n",
    "print(execute_query(postgres_df, [\n",
    "      \"CustomerID\", \"CustomerName\", \"Contact\",\"Waste\"], (\"AreaType\", \"residential\")))\n",
    "print(execute_query(mysql_df, [\"CustomerName\",\n",
    "      \"ContractID\", \"ContractStartDate\"], ('Reason', 'Late Payment')))\n",
    "\"\"\" print(execute_query(web_df, [\"WaterResourceID\",\"Turbidity\", \"phlevel\",], (\"Reservoir\", \"Reservoir_1\"))) \"\"\"\n",
    "\n",
    "\n",
    "def execute_lav_query(df, group_by_col, agg_col, agg_func=\"mean\"):\n",
    "    try:\n",
    "        actual_group_by_col = get_actual_column(df, group_by_col)\n",
    "        actual_agg_col = get_actual_column(df, agg_col)\n",
    "\n",
    "        if actual_group_by_col and actual_agg_col:\n",
    "            result = df.groupby(actual_group_by_col)[\n",
    "                actual_agg_col].agg(agg_func).reset_index()\n",
    "            return result\n",
    "        else:\n",
    "            print(f\"Warning: Column {group_by_col} or {agg_col} not found.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing LAV query: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Execute LAV query for average Turbidity per Reservoir\n",
    "print(\"\\nExecuting LAV Query for Average Turbidity per Reservoir:\")\n",
    "print(execute_lav_query(web_df, \"Reservoir\", \"Turbidity\", \"mean\"))\n",
    "# Simulate GLAV queries using pandas merge (join) with corrected column names\n",
    "print(\"\\nExecuting GLAV Queries:\")\n",
    "\n",
    "\"\"\" # Fix GLAV1 Join\n",
    "glav1 = pd.merge(\n",
    "    cassandra_df,\n",
    "    mongo_df,\n",
    "    left_on=get_actual_column(cassandra_df, \"CustomerID\"),\n",
    "    right_on=get_actual_column(mongo_df, \"CustomerID\"),\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(\"GLAV1 Debug Columns:\", glav1.columns.tolist())\n",
    "actual_cols1 = [get_actual_column(glav1, col) for col in [\n",
    "    \"CustomerID\", \"CustomerName\", \"Contact\"] if get_actual_column(glav1, col)]\n",
    "if actual_cols1:\n",
    "    print(glav1[actual_cols1].head())\n",
    "else:\n",
    "    print(\"GLAV1: No matching columns found.\") \"\"\"\n",
    "\n",
    "# Fix GLAV2 Join\n",
    "glav2 = pd.merge(\n",
    "    postgres_df,\n",
    "    mysql_df,\n",
    "    left_on=get_actual_column(postgres_df, \"CustomerID\"),\n",
    "    right_on=get_actual_column(mysql_df, \"CustomerID\"),\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(\"GLAV2 Debug Columns:\", glav2.columns.tolist())\n",
    "actual_cols2 = [get_actual_column(glav2, col) for col in [\n",
    "    \"CustomerID\", \"CustomerName\"] if get_actual_column(glav2, col)]\n",
    "if actual_cols2:\n",
    "    print(glav2[actual_cols2].head())\n",
    "else:\n",
    "    print(\"GLAV2: No matching columns found.\")\n",
    "\"\"\" # Debugging: Print Available Columns Before Merging\n",
    "print(\"\\nXML DataFrame Columns:\", xml_df.columns.tolist())\n",
    "print(\"Web DataFrame Columns:\", web_df.columns.tolist()) \"\"\"\n",
    "\n",
    "\"\"\" # Find actual column names before merging\n",
    "xml_customer_col = get_actual_column(xml_df, \"CustomerID\")\n",
    "web_customer_col = get_actual_column(web_df, \"CustomerID\")\n",
    "\n",
    "# Check if both columns exist before merging\n",
    "if xml_customer_col and web_customer_col:\n",
    "    glav3 = pd.merge(xml_df, web_df, left_on=xml_customer_col,\n",
    "                     right_on=web_customer_col, how=\"inner\")\n",
    "    print(\"GLAV3 Debug Columns:\", glav3.columns.tolist())\n",
    "\n",
    "    # Select columns dynamically\n",
    "    actual_cols3 = [get_actual_column(glav3, col) for col in [\n",
    "        \"CustomerID\", \"Address\"] if get_actual_column(glav3, col)]\n",
    "    if actual_cols3:\n",
    "        print(glav3[actual_cols3].head())\n",
    "    else:\n",
    "        print(\"GLAV3: No matching columns found.\")\n",
    "else:\n",
    "    print(\"\\n GLAV3 Merge Skipped: Could not find common columns in XML and Web DataFrames!\")\n",
    "    print(f\"XML Customer Column: {xml_customer_col}, Web Customer Column: {web_customer_col}\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clientid</th>\n",
       "      <th>name</th>\n",
       "      <th>fulladdress</th>\n",
       "      <th>postcode</th>\n",
       "      <th>areatypeid</th>\n",
       "      <th>mobileinfo</th>\n",
       "      <th>areatype</th>\n",
       "      <th>description_x</th>\n",
       "      <th>billingid_x</th>\n",
       "      <th>billingdate_x</th>\n",
       "      <th>...</th>\n",
       "      <th>equipmentname</th>\n",
       "      <th>lastinspectiondate</th>\n",
       "      <th>logid</th>\n",
       "      <th>maintenancedate</th>\n",
       "      <th>partsreplaced</th>\n",
       "      <th>techniciannotes</th>\n",
       "      <th>billingid_y</th>\n",
       "      <th>billingdate_y</th>\n",
       "      <th>amountdue</th>\n",
       "      <th>paymentstatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Customer_1</td>\n",
       "      <td>rua 1, Faro</td>\n",
       "      <td>801-00001</td>\n",
       "      <td>1</td>\n",
       "      <td>35191000001</td>\n",
       "      <td>residential</td>\n",
       "      <td>Areas primarily for housing and residential pu...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-31</td>\n",
       "      <td>...</td>\n",
       "      <td>Treatment Equipment 1</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-07</td>\n",
       "      <td>Part_2</td>\n",
       "      <td>Technician note 1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>75.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Customer_1</td>\n",
       "      <td>rua 1, Faro</td>\n",
       "      <td>801-00001</td>\n",
       "      <td>1</td>\n",
       "      <td>35191000001</td>\n",
       "      <td>residential</td>\n",
       "      <td>Areas primarily for housing and residential pu...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-31</td>\n",
       "      <td>...</td>\n",
       "      <td>Treatment Equipment 1</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>51</td>\n",
       "      <td>2020-07-06</td>\n",
       "      <td>Part_2</td>\n",
       "      <td>Technician note 51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>75.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Customer_1</td>\n",
       "      <td>rua 1, Faro</td>\n",
       "      <td>801-00001</td>\n",
       "      <td>1</td>\n",
       "      <td>35191000001</td>\n",
       "      <td>residential</td>\n",
       "      <td>Areas primarily for housing and residential pu...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-31</td>\n",
       "      <td>...</td>\n",
       "      <td>Treatment Equipment 1</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>101</td>\n",
       "      <td>2020-12-03</td>\n",
       "      <td>Part_2</td>\n",
       "      <td>Technician note 101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>75.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Customer_1</td>\n",
       "      <td>rua 1, Faro</td>\n",
       "      <td>801-00001</td>\n",
       "      <td>1</td>\n",
       "      <td>35191000001</td>\n",
       "      <td>residential</td>\n",
       "      <td>Areas primarily for housing and residential pu...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-31</td>\n",
       "      <td>...</td>\n",
       "      <td>Treatment Equipment 1</td>\n",
       "      <td>2020-02-05</td>\n",
       "      <td>151</td>\n",
       "      <td>2020-05-02</td>\n",
       "      <td>Part_2</td>\n",
       "      <td>Technician note 151</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>75.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Customer_1</td>\n",
       "      <td>rua 1, Faro</td>\n",
       "      <td>801-00001</td>\n",
       "      <td>1</td>\n",
       "      <td>35191000001</td>\n",
       "      <td>residential</td>\n",
       "      <td>Areas primarily for housing and residential pu...</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-01-31</td>\n",
       "      <td>...</td>\n",
       "      <td>Treatment Equipment 6</td>\n",
       "      <td>2020-02-10</td>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-22</td>\n",
       "      <td>Part_2</td>\n",
       "      <td>Technician note 6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-03-06</td>\n",
       "      <td>75.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   clientid        name  fulladdress   postcode  areatypeid   mobileinfo  \\\n",
       "0         1  Customer_1  rua 1, Faro  801-00001           1  35191000001   \n",
       "1         1  Customer_1  rua 1, Faro  801-00001           1  35191000001   \n",
       "2         1  Customer_1  rua 1, Faro  801-00001           1  35191000001   \n",
       "3         1  Customer_1  rua 1, Faro  801-00001           1  35191000001   \n",
       "4         1  Customer_1  rua 1, Faro  801-00001           1  35191000001   \n",
       "\n",
       "      areatype                                      description_x  \\\n",
       "0  residential  Areas primarily for housing and residential pu...   \n",
       "1  residential  Areas primarily for housing and residential pu...   \n",
       "2  residential  Areas primarily for housing and residential pu...   \n",
       "3  residential  Areas primarily for housing and residential pu...   \n",
       "4  residential  Areas primarily for housing and residential pu...   \n",
       "\n",
       "   billingid_x billingdate_x  ...          equipmentname  lastinspectiondate  \\\n",
       "0            1    2025-01-31  ...  Treatment Equipment 1          2020-02-05   \n",
       "1            1    2025-01-31  ...  Treatment Equipment 1          2020-02-05   \n",
       "2            1    2025-01-31  ...  Treatment Equipment 1          2020-02-05   \n",
       "3            1    2025-01-31  ...  Treatment Equipment 1          2020-02-05   \n",
       "4            1    2025-01-31  ...  Treatment Equipment 6          2020-02-10   \n",
       "\n",
       "   logid  maintenancedate  partsreplaced      techniciannotes  billingid_y  \\\n",
       "0      1       2020-02-07         Part_2    Technician note 1          1.0   \n",
       "1     51       2020-07-06         Part_2   Technician note 51          1.0   \n",
       "2    101       2020-12-03         Part_2  Technician note 101          1.0   \n",
       "3    151       2020-05-02         Part_2  Technician note 151          1.0   \n",
       "4      6       2020-02-22         Part_2    Technician note 6          1.0   \n",
       "\n",
       "  billingdate_y  amountdue paymentstatus  \n",
       "0    2020-03-06       75.5           0.0  \n",
       "1    2020-03-06       75.5           0.0  \n",
       "2    2020-03-06       75.5           0.0  \n",
       "3    2020-03-06       75.5           0.0  \n",
       "4    2020-03-06       75.5           0.0  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glav2.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
